{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tyler.merp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tyler.merp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined_text</th>\n",
       "      <th>link_flair_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for everyone, whether you're worried about cov...</td>\n",
       "      <td>psa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and people wonder why the virus spreads…</td>\n",
       "      <td>rant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r/udub mods</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thoughts on madrona? i have an emotional suppo...</td>\n",
       "      <td>discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>soc 222 anyone has took or taking soc222(socio...</td>\n",
       "      <td>academics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       combined_text link_flair_text\n",
       "0  for everyone, whether you're worried about cov...             psa\n",
       "1          and people wonder why the virus spreads…             rant\n",
       "2                                       r/udub mods             meme\n",
       "3  thoughts on madrona? i have an emotional suppo...      discussion\n",
       "4  soc 222 anyone has took or taking soc222(socio...       academics"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = pd.read_csv('AllDataFiltered.csv')\n",
    "posts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "posts['tokens'] = posts['combined_text'].apply(clean_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tyler.merp/anaconda3/envs/INFO492/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=lambda x: x, lowercase=False, max_features=5000)\n",
    "\n",
    "texts_bow = count_vectorizer.fit_transform(posts['tokens'])\n",
    "\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(texts_bow, posts['link_flair_text'], test_size=0.2, random_state=52)\n",
    "\n",
    "clf_bow = MultinomialNB()\n",
    "clf_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred_bow = clf_bow.predict(X_test_bow)\n",
    "\n",
    "accuracy_bow = accuracy_score(y_test, y_pred_bow)\n",
    "\n",
    "results.append({'vectorizationType': 'Bag of Words (BoW)', 'Dataset': 'AllDataFiltered.csv', 'Accuracy': accuracy_bow * 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tyler.merp/anaconda3/envs/INFO492/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False, max_features=5000)\n",
    "\n",
    "texts_tfidf = tfidf_vectorizer.fit_transform(posts['tokens'])\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(texts_tfidf, posts['link_flair_text'], test_size=0.2, random_state=52)\n",
    "\n",
    "clf_tfidf = MultinomialNB()\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "results.append({'vectorizationType': 'TF-IDF', 'Dataset': 'AllDataFiltered.csv', 'Accuracy': accuracy_tfidf * 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = posts[\"combined_text\"].values\n",
    "tokenized_docs = posts[\"tokens\"].values\n",
    "model = Word2Vec(sentences=tokenized_docs, window = 2, vector_size=200, workers=1, seed=52)\n",
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n",
    "    \n",
    "vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_docs, posts['link_flair_text'], test_size=0.2, random_state=52)\n",
    "clf = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy_word2vec = accuracy_score(y_test, y_pred)\n",
    "\n",
    "results.append({'vectorizationType': 'Word2Vec', 'Dataset': 'AllDataFiltered.csv', 'Accuracy': accuracy_word2vec * 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    vectorizationType              Dataset   Accuracy\n",
      "0  Bag of Words (BoW)  AllDataFiltered.csv  53.519131\n",
      "1              TF-IDF  AllDataFiltered.csv  48.984412\n",
      "2            Word2Vec  AllDataFiltered.csv  40.576287\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INFO492",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
